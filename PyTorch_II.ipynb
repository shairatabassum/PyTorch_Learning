{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch II.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN2GpO1aENgtEfYuuoRNbVX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shairatabassum/PyTorch_Learning/blob/main/PyTorch_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eom708rxHbqz"
      },
      "source": [
        "# **Deep Learning with PyTorch** (part II)\n",
        "A PyTorch notebook that I created while learning PyTorch through a series of tutorials on YouTube.\n",
        "\n",
        "Source: [PyTorch Tutorials - Complete Beginner Course](https://www.youtube.com/watch?v=EMXfZB8FVUA&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4)\n",
        "\n",
        "Dataset: [Wine](https://github.com/python-engineer/pytorchTutorial/blob/master/data/wine/wine.csv)\n",
        "\n",
        "> **Table of contents**\n",
        "> *  Linear Regression\n",
        "> *  Logistic Regression\n",
        "> *  Dataset and DataLoader\n",
        "> *  Dataset Transforms\n",
        "> *  Softmax Function and Cross-Entropy Loss\n",
        "> *  A Neural Network Example\n",
        "> *  Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91THTJIJhAIj"
      },
      "source": [
        "### **Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcX7wFD0HY8U"
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaroImRJgsFZ"
      },
      "source": [
        "# prepare data\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
        "\n",
        "# convert into torch tensors\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "\n",
        "#reshape y tensor\n",
        "y = y.view(y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzElbCP5rwD_",
        "outputId": "937ae236-8ae4-4278-a021-42f7afdcb5c7"
      },
      "source": [
        "# design the model\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# define the loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss() # mean square error loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # stochastic gradient descent\n",
        "\n",
        "# training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # forward pass and loss\n",
        "  y_pred = model(X)\n",
        "  loss = criterion(y_pred, y)\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if((epoch+1) % 10 == 0):\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 10, loss = 4372.7114\n",
            "epoch: 20, loss = 3262.1604\n",
            "epoch: 30, loss = 2458.8010\n",
            "epoch: 40, loss = 1877.0297\n",
            "epoch: 50, loss = 1455.3043\n",
            "epoch: 60, loss = 1149.3115\n",
            "epoch: 70, loss = 927.1013\n",
            "epoch: 80, loss = 765.6057\n",
            "epoch: 90, loss = 648.1499\n",
            "epoch: 100, loss = 562.6675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "1lbB6RAGs3Tj",
        "outputId": "e4005608-462d-4064-c9c7-01edc256ed78"
      },
      "source": [
        "# plot\n",
        "pred = model(X).detach().numpy()\n",
        "plt.plot(X_numpy, y_numpy, 'ro')\n",
        "plt.plot(X_numpy, pred, 'b')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDmO97l8uRt3"
      },
      "source": [
        "### **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs8z0kMHttrU"
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPy1tVUOvSgA",
        "outputId": "ed5d17a1-056e-4c30-cf31-b95b9c2b17a5"
      },
      "source": [
        "# prepare data (Breast Cancer Dataset from sklearn)\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "569 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVCH4QA4v2bC"
      },
      "source": [
        "# split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# scale the features (it makes the features to have zero means and unit variance)\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# convert into torch tensors\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# reshape y tensors\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh026AraxnGE",
        "outputId": "93ad7e97-23cd-4db5-ac7e-28b26964cf22"
      },
      "source": [
        "# design the model\n",
        "class LogisticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = torch.sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "# define the loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss() # binary cross entropy loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # stochastic gradient descent\n",
        "\n",
        "# training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # forward pass and loss\n",
        "  y_pred = model(X_train)\n",
        "  loss = criterion(y_pred, y_train)\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if((epoch+1) % 10 == 0):\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 10, loss = 0.6199\n",
            "epoch: 20, loss = 0.5016\n",
            "epoch: 30, loss = 0.4271\n",
            "epoch: 40, loss = 0.3759\n",
            "epoch: 50, loss = 0.3386\n",
            "epoch: 60, loss = 0.3101\n",
            "epoch: 70, loss = 0.2877\n",
            "epoch: 80, loss = 0.2694\n",
            "epoch: 90, loss = 0.2542\n",
            "epoch: 100, loss = 0.2414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qks2Qk1gyqOS",
        "outputId": "67a9307a-97fd-4890-f0eb-a3a5e36986b7"
      },
      "source": [
        "# model evaluation\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test)\n",
        "  y_pred_cls = y_pred.round() # convert class labels in 0 and 1\n",
        "  accuracy = y_pred_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'accuracy = {accuracy:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 0.9211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vetpn8B2UrBx"
      },
      "source": [
        "### **Dataset and DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ljnAma1m3W"
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cByOv3bgU-He"
      },
      "source": [
        "class WineDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    xy = np.loadtxt('wine.csv', delimiter=\",\", dtype=np.float32, skiprows=1)\n",
        "    self.x = torch.from_numpy(xy[:, 1:]) # all rows, all columns except the first one\n",
        "    self.y = torch.from_numpy(xy[:, [0]]) # all rows, only first column\n",
        "    self.n_samples = xy.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # call dataset with an index\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    # call length of dataset\n",
        "    return self.n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8G5np7gHAc",
        "outputId": "87a808da-1bf8-41ff-f62d-69400ebe91e8"
      },
      "source": [
        "# Dataset\n",
        "dataset = WineDataset()\n",
        "first_data = dataset[0] # first sample\n",
        "features, labels = first_data\n",
        "print(features, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03]) tensor([1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORr253lplFHd",
        "outputId": "0ec1ff76-05ce-40ad-ef92-54d8e5b2feeb"
      },
      "source": [
        "# DataLoader\n",
        "dataset = WineDataset()\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "dataiter = iter(dataloader)\n",
        "data = dataiter.next()\n",
        "features, labels = data\n",
        "print(features, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.3870e+01, 1.9000e+00, 2.8000e+00, 1.9400e+01, 1.0700e+02, 2.9500e+00,\n",
            "         2.9700e+00, 3.7000e-01, 1.7600e+00, 4.5000e+00, 1.2500e+00, 3.4000e+00,\n",
            "         9.1500e+02],\n",
            "        [1.3490e+01, 3.5900e+00, 2.1900e+00, 1.9500e+01, 8.8000e+01, 1.6200e+00,\n",
            "         4.8000e-01, 5.8000e-01, 8.8000e-01, 5.7000e+00, 8.1000e-01, 1.8200e+00,\n",
            "         5.8000e+02],\n",
            "        [1.3840e+01, 4.1200e+00, 2.3800e+00, 1.9500e+01, 8.9000e+01, 1.8000e+00,\n",
            "         8.3000e-01, 4.8000e-01, 1.5600e+00, 9.0100e+00, 5.7000e-01, 1.6400e+00,\n",
            "         4.8000e+02],\n",
            "        [1.2850e+01, 3.2700e+00, 2.5800e+00, 2.2000e+01, 1.0600e+02, 1.6500e+00,\n",
            "         6.0000e-01, 6.0000e-01, 9.6000e-01, 5.5800e+00, 8.7000e-01, 2.1100e+00,\n",
            "         5.7000e+02]]) tensor([[1.],\n",
            "        [3.],\n",
            "        [3.],\n",
            "        [3.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH-QPs6dlHzg",
        "outputId": "bda3a265-c0ec-4d9b-f582-b1d19d000c24"
      },
      "source": [
        "# dummy training loop\n",
        "num_epochs = 2\n",
        "total_samples = len(dataset)\n",
        "n_iterations = math.ceil(total_samples/4)\n",
        "print(total_samples, n_iterations)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (inputs, labels) in enumerate(dataloader):\n",
        "    # forward pass\n",
        "    # backward pass\n",
        "    # update weights\n",
        "    if(i+1) % 5 == 0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "178 45\n",
            "epoch 1/2, step 5/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 10/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 15/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 20/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 25/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 30/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 35/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 40/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 45/45, inputs torch.Size([2, 13])\n",
            "epoch 2/2, step 5/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 10/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 15/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 20/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 25/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 30/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 35/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 40/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 45/45, inputs torch.Size([2, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq--nbZHrRYV"
      },
      "source": [
        "### **Dataset Transforms**\n",
        "Transforms can be applied to PIL imgaes, tensors, or custom data during creation of the Dataset. Complete list of built-in transforms:\n",
        "[torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)\n",
        "\n",
        "**On Images:** CenterCrop, Grayscale, Pad, RandomAffine, RandomCrop, RandomHorizontalFlip, RandomRotation, Resize, Scale\n",
        "\n",
        "**On Tensors:** LinearTransformation, Normalize, RandomErasing\n",
        "\n",
        "**Conversion:** from tensor to ndarray (to PILImage), from numpy.ndarray to PILImage (to tensor)\n",
        "\n",
        "**Generic:** Use Lambda\n",
        "\n",
        "**Custom:** Write own class\n",
        "\n",
        "**Compose multiple Transforms:** \n",
        "```\n",
        "composed = transforms.Compose([Rescale(256), RandomCrop(224)])\n",
        "torchvision.transforms.ReScale(256)\n",
        "torchvision.transforms.ToTensor()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8N962skoPhX",
        "outputId": "71c9d52d-e3a0-4ac5-c8ee-7407d83ad3c7"
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "  def __init__(self, transform=None):\n",
        "    # data loading\n",
        "    xy = np.loadtxt('wine.csv', delimiter=\",\", dtype=np.float32, skiprows=1)\n",
        "    self.n_samples = xy.shape[0]\n",
        "    self.x = xy[:, 1:] # all rows, all columns except the first one\n",
        "    self.y = xy[:, [0]] # all rows, only first column\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # call dataset with an index\n",
        "    sample = self.x[index], self.y[index]\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    # call length of dataset\n",
        "    return self.n_samples\n",
        "\n",
        "# custom transform\n",
        "class ToTensor:\n",
        "  def __call__(self, sample):\n",
        "    inputs, targets = sample\n",
        "    return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "dataset = WineDataset(transform=ToTensor())\n",
        "first_data = dataset[0] # first sample\n",
        "features, labels = first_data\n",
        "print(features)\n",
        "print(type(features), type(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7o3aOw3vE0N",
        "outputId": "e6f950a6-048a-4b2c-c518-72122caf7a24"
      },
      "source": [
        "# custom transform\n",
        "class MulTransform:\n",
        "  def __init__(self, factor):\n",
        "    self.factor = factor\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    inputs, target = sample\n",
        "    inputs *= self.factor\n",
        "    return inputs, target\n",
        "\n",
        "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(4)])\n",
        "dataset = WineDataset(transform=composed)\n",
        "first_data = dataset[0] # first sample\n",
        "features, labels = first_data\n",
        "print(features)\n",
        "print(type(features), type(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n",
            "        1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n",
            "        4.2600e+03])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDdU2f9W-0fw"
      },
      "source": [
        "### **Softmax Function and Cross-Entropy Loss**\n",
        "\n",
        "**Softmax function** applies an exponential function to each element and then normalizes it by dividing by the sum of all these exponentials. It basically squashes the output to be between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSlQMcYEwIgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8f5567-ab4e-4670-e7a4-4fcee498b784"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "# softmax in numpy\n",
        "x = np.array([2.0, 1.0, 0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy:', outputs)\n",
        "\n",
        "# softmax in pytorch\n",
        "x = torch.tensor([2.0, 1.0, 0.1])\n",
        "outputs = torch.softmax(x, dim=0)\n",
        "print('softmax pytorch:', outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
            "softmax pytorch: tensor([0.6590, 0.2424, 0.0986])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3kU7xhvAebQ"
      },
      "source": [
        "The **softmax function** can be combined with the **cross-entropy loss**. This measures the performance of the classification model whose output is a probability between 0 and 1. It can be used in multi-class problems.\n",
        "\n",
        "The loss increases as the predicted probability diverges from the actual level. So, the better is the prediction, the lower is the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk5M755w_6Ma",
        "outputId": "4cc1917e-c001-4a31-9046-224a9e401bf3"
      },
      "source": [
        "# cross-entropy loss in numpy\n",
        "\n",
        "def cross_entropy(actual, predicted):\n",
        "  loss = -np.sum(actual * np.log(predicted))\n",
        "  return loss\n",
        "\n",
        "# y must be one-hot encoded\n",
        "Y = np.array([1, 0, 0])\n",
        "\n",
        "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
        "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
        "l1 = cross_entropy(Y, Y_pred_good)\n",
        "l2 = cross_entropy(Y, Y_pred_bad)\n",
        "print(f'Loss1 numpy: {l1:.4f}')\n",
        "print(f'Loss1 numpy: {l2:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1 numpy: 0.3567\n",
            "Loss1 numpy: 2.3026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NncqHqN1DUnl"
      },
      "source": [
        "**Things to consider when implementing Cross-entropy Loss in PyTorch:**\n",
        "\n",
        "*  **nn.CrossEntropyLoss** applies **nn.LogSoftmax** and **nn.NLLLoss** (negative log likelihood loss together. So, we should not implement softmax by ourselves in the last layer. \n",
        "*  **Y** must not be one-hot encoded, we have to put class labels here.\n",
        "*  **Y_pred** has raw scores (logits), so no Softmax!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SvuQJDcB82W",
        "outputId": "13a456ec-9a67-4834-b1e0-b236ae82b715"
      },
      "source": [
        "# cross-entropy loss in pytorch\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "Y = torch.tensor([0])\n",
        "\n",
        "# nsamples x nclasses = 1 x 3\n",
        "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\n",
        "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
        "\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "\n",
        "print(l1.item())\n",
        "print(l2.item())\n",
        "\n",
        "# to get the actual values of predictions\n",
        "_, predictions1 = torch.max(Y_pred_good, 1)\n",
        "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
        "print(predictions1)\n",
        "print(predictions2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4170299470424652\n",
            "1.840616226196289\n",
            "tensor([0])\n",
            "tensor([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAfvQHQZExie",
        "outputId": "e820b06a-b973-447d-888e-7fa5b0df4792"
      },
      "source": [
        "# loss in pytorch allows multiple classes\n",
        "\n",
        "# three possible classes\n",
        "Y = torch.tensor([2, 0, 1])\n",
        "\n",
        "# nsamples x nclasses = 3 x 3\n",
        "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1], [2.0, 1.0, 0.1], [0.1, 3.0, 0.1]])\n",
        "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1], [0.1, 1.0, 2.1], [0.1, 3.0, 0.1]])\n",
        "\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "\n",
        "print(l1.item())\n",
        "print(l2.item())\n",
        "\n",
        "# to get the actual values of predictions\n",
        "_, predictions1 = torch.max(Y_pred_good, 1)\n",
        "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
        "print(predictions1)\n",
        "print(predictions2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3018244206905365\n",
            "1.6241613626480103\n",
            "tensor([2, 0, 1])\n",
            "tensor([0, 2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be2RxPlzMMCY"
      },
      "source": [
        "### **A Neural Network Example**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D476yHk9G7KY"
      },
      "source": [
        "# multi class problem\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet2, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.linear1(x)\n",
        "      out = self.relu(out)\n",
        "      out = self.linear2(out)\n",
        "      return outputs\n",
        "\n",
        "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
        "criterion = nn.CrossEntropyLoss() # applies softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJo4KFWgNlOH"
      },
      "source": [
        "# binary classification\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet2, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.linear1(x)\n",
        "      out = self.relu(out)\n",
        "      out = self.linear2(out)\n",
        "      y_pred = torch.sigmoid(out)\n",
        "      return y_pred\n",
        "\n",
        "model = NeuralNet2(input_size=28*28, hidden_size=5)\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8mGlbOIXoD4"
      },
      "source": [
        "### **Activation Functions**\n",
        "Activation functions apply a non-linear transformation to the layer output and basically decide whether a neuron should be activated or not. \n",
        "\n",
        "**Why do we use them? Why is only a linear transformation not enough?**\n",
        "\n",
        "Typically, a linear layer in the network applies linear transformation which multiplies the input with some weights and delivers the output. Suppose, there are no activation functions in between, then we would have only linear transformations after each layer. So, our whole network from input to output is just a linear regression model which is not suited for more complex tasks. \n",
        "\n",
        "Therefore, with non-linear transformations in between, our network can learn better and perform more complex tasks. \n",
        "\n",
        "**Most popular activation functions:**\n",
        "1. **Step function:** It will just make the output one if the input is greater than a threshold value. This function is not usually used in practice.\n",
        "2. **Sigmoid:** Sigmoid function is a popular choice. It gives output a probability between 0 and 1. It is typically used in the last layer of a binary classification problem.\n",
        "3. **TanH:** This is basically a scaled sigmoid function and also a little bit shifted. So, this will give output a value between -1 and +1. It is a good choice in hidden layers.\n",
        "4. **ReLU:** Most popular choice in most of the networks. This will give output 0 for negative values and output simply the input for positive values. If you don't know what to use, just use a ReLU for hidden layers. \n",
        "5. **Leaky ReLU:** Slightly modified and improved version of the ReLU. This will still give output the input when X is positive, but it will multiply the input with a very small value when X is negative. This function tries to solve the vanishing gradient problem. In a normal ReLU where the negative values are zero, the gradient later in the backpropagation will also be zero. If the gradient is zero then the weights will never be updated. Thus, the neurons will be dead as they won't learn anything. This is why, sometimes we can use Leaky ReLU function instead of ReLU. \n",
        "6. **Softmax:** It basically squash the inputs to be outputs between 0 and 1, so that we have a probability as an output. This is a good choice in the last layer in multi class classification problems. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54xD0ApHOy6W"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# option 1 (create nn modules)\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.linear1(x)\n",
        "      out = self.relu(out)\n",
        "      out = self.linear2(out)\n",
        "      out = torch.sigmoid(out)\n",
        "      return out\n",
        "\n",
        "# option 2 (use activation functions directly in forward pass)\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = torch.relu(self.linear1(x))\n",
        "      out = torch.sigmoid(self.linear2(out))\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}